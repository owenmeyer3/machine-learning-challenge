{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the CSV and Perform Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  koi_disposition  koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  \\\n",
       "0       CONFIRMED              0              0              0              0   \n",
       "1  FALSE POSITIVE              0              1              0              0   \n",
       "2  FALSE POSITIVE              0              1              0              0   \n",
       "3       CONFIRMED              0              0              0              0   \n",
       "4       CONFIRMED              0              0              0              0   \n",
       "\n",
       "   koi_period  koi_period_err1  koi_period_err2  koi_time0bk  \\\n",
       "0   54.418383     2.479000e-04    -2.479000e-04   162.513840   \n",
       "1   19.899140     1.490000e-05    -1.490000e-05   175.850252   \n",
       "2    1.736952     2.630000e-07    -2.630000e-07   170.307565   \n",
       "3    2.525592     3.760000e-06    -3.760000e-06   171.595550   \n",
       "4    4.134435     1.050000e-05    -1.050000e-05   172.979370   \n",
       "\n",
       "   koi_time0bk_err1  ...  koi_steff_err2  koi_slogg  koi_slogg_err1  \\\n",
       "0          0.003520  ...             -81      4.467           0.064   \n",
       "1          0.000581  ...            -176      4.544           0.044   \n",
       "2          0.000115  ...            -174      4.564           0.053   \n",
       "3          0.001130  ...            -211      4.438           0.070   \n",
       "4          0.001900  ...            -232      4.486           0.054   \n",
       "\n",
       "   koi_slogg_err2  koi_srad  koi_srad_err1  koi_srad_err2         ra  \\\n",
       "0          -0.096     0.927          0.105         -0.061  291.93423   \n",
       "1          -0.176     0.868          0.233         -0.078  297.00482   \n",
       "2          -0.168     0.791          0.201         -0.067  285.53461   \n",
       "3          -0.210     1.046          0.334         -0.133  288.75488   \n",
       "4          -0.229     0.972          0.315         -0.105  296.28613   \n",
       "\n",
       "         dec  koi_kepmag  \n",
       "0  48.141651      15.347  \n",
       "1  48.134129      15.436  \n",
       "2  48.285210      15.597  \n",
       "3  48.226200      15.509  \n",
       "4  48.224670      15.714  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>koi_disposition</th>\n      <th>koi_fpflag_nt</th>\n      <th>koi_fpflag_ss</th>\n      <th>koi_fpflag_co</th>\n      <th>koi_fpflag_ec</th>\n      <th>koi_period</th>\n      <th>koi_period_err1</th>\n      <th>koi_period_err2</th>\n      <th>koi_time0bk</th>\n      <th>koi_time0bk_err1</th>\n      <th>...</th>\n      <th>koi_steff_err2</th>\n      <th>koi_slogg</th>\n      <th>koi_slogg_err1</th>\n      <th>koi_slogg_err2</th>\n      <th>koi_srad</th>\n      <th>koi_srad_err1</th>\n      <th>koi_srad_err2</th>\n      <th>ra</th>\n      <th>dec</th>\n      <th>koi_kepmag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CONFIRMED</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>54.418383</td>\n      <td>2.479000e-04</td>\n      <td>-2.479000e-04</td>\n      <td>162.513840</td>\n      <td>0.003520</td>\n      <td>...</td>\n      <td>-81</td>\n      <td>4.467</td>\n      <td>0.064</td>\n      <td>-0.096</td>\n      <td>0.927</td>\n      <td>0.105</td>\n      <td>-0.061</td>\n      <td>291.93423</td>\n      <td>48.141651</td>\n      <td>15.347</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FALSE POSITIVE</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19.899140</td>\n      <td>1.490000e-05</td>\n      <td>-1.490000e-05</td>\n      <td>175.850252</td>\n      <td>0.000581</td>\n      <td>...</td>\n      <td>-176</td>\n      <td>4.544</td>\n      <td>0.044</td>\n      <td>-0.176</td>\n      <td>0.868</td>\n      <td>0.233</td>\n      <td>-0.078</td>\n      <td>297.00482</td>\n      <td>48.134129</td>\n      <td>15.436</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FALSE POSITIVE</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.736952</td>\n      <td>2.630000e-07</td>\n      <td>-2.630000e-07</td>\n      <td>170.307565</td>\n      <td>0.000115</td>\n      <td>...</td>\n      <td>-174</td>\n      <td>4.564</td>\n      <td>0.053</td>\n      <td>-0.168</td>\n      <td>0.791</td>\n      <td>0.201</td>\n      <td>-0.067</td>\n      <td>285.53461</td>\n      <td>48.285210</td>\n      <td>15.597</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CONFIRMED</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.525592</td>\n      <td>3.760000e-06</td>\n      <td>-3.760000e-06</td>\n      <td>171.595550</td>\n      <td>0.001130</td>\n      <td>...</td>\n      <td>-211</td>\n      <td>4.438</td>\n      <td>0.070</td>\n      <td>-0.210</td>\n      <td>1.046</td>\n      <td>0.334</td>\n      <td>-0.133</td>\n      <td>288.75488</td>\n      <td>48.226200</td>\n      <td>15.509</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CONFIRMED</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.134435</td>\n      <td>1.050000e-05</td>\n      <td>-1.050000e-05</td>\n      <td>172.979370</td>\n      <td>0.001900</td>\n      <td>...</td>\n      <td>-232</td>\n      <td>4.486</td>\n      <td>0.054</td>\n      <td>-0.229</td>\n      <td>0.972</td>\n      <td>0.315</td>\n      <td>-0.105</td>\n      <td>296.28613</td>\n      <td>48.224670</td>\n      <td>15.714</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 41 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"exoplanet_data.csv\")\n",
    "# Drop the null columns where all values are null\n",
    "df = df.dropna(axis='columns', how='all')\n",
    "# Drop the null rows\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select your features (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features. This will also be used as your x values. Try all features first\n",
    "X = df.drop('koi_disposition', axis=1)\n",
    "y = df['koi_disposition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Train Test Split\n",
    "\n",
    "Use `koi_disposition` for the y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  koi_period  \\\n",
       "3563              0              0              0              0   10.548413   \n",
       "4099              0              0              0              0   24.754385   \n",
       "5460              0              0              0              0    1.057336   \n",
       "1091              0              0              0              0  201.118319   \n",
       "5999              0              0              0              0   91.649983   \n",
       "\n",
       "      koi_period_err1  koi_period_err2  koi_time0bk  koi_time0bk_err1  \\\n",
       "3563     5.470000e-05    -5.470000e-05   139.064020          0.004110   \n",
       "4099     1.365000e-04    -1.365000e-04   140.207320          0.004460   \n",
       "5460     1.230000e-07    -1.230000e-07   131.792007          0.000096   \n",
       "1091     1.461000e-03    -1.461000e-03   187.569860          0.005290   \n",
       "5999     3.181000e-03    -3.181000e-03   175.715600          0.028600   \n",
       "\n",
       "      koi_time0bk_err2  ...  koi_steff_err2  koi_slogg  koi_slogg_err1  \\\n",
       "3563         -0.004110  ...            -133      4.387           0.066   \n",
       "4099         -0.004460  ...            -144      4.519           0.078   \n",
       "5460         -0.000096  ...            -140      4.594           0.054   \n",
       "1091         -0.005290  ...            -112      4.447           0.072   \n",
       "5999         -0.028600  ...            -233      4.145           0.164   \n",
       "\n",
       "      koi_slogg_err2  koi_srad  koi_srad_err1  koi_srad_err2         ra  \\\n",
       "3563          -0.123     1.092          0.181         -0.097  298.09543   \n",
       "4099          -0.052     0.804          0.056         -0.076  295.73535   \n",
       "5460          -0.027     0.683          0.054         -0.060  292.18417   \n",
       "1091          -0.108     0.954          0.135         -0.083  283.11377   \n",
       "5999          -0.164     1.608          0.905         -0.383  294.93198   \n",
       "\n",
       "            dec  koi_kepmag  \n",
       "3563  44.737061      13.204  \n",
       "4099  42.576248      15.514  \n",
       "5460  49.310040      15.414  \n",
       "1091  48.131390      13.328  \n",
       "5999  39.812420      12.964  \n",
       "\n",
       "[5 rows x 40 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>koi_fpflag_nt</th>\n      <th>koi_fpflag_ss</th>\n      <th>koi_fpflag_co</th>\n      <th>koi_fpflag_ec</th>\n      <th>koi_period</th>\n      <th>koi_period_err1</th>\n      <th>koi_period_err2</th>\n      <th>koi_time0bk</th>\n      <th>koi_time0bk_err1</th>\n      <th>koi_time0bk_err2</th>\n      <th>...</th>\n      <th>koi_steff_err2</th>\n      <th>koi_slogg</th>\n      <th>koi_slogg_err1</th>\n      <th>koi_slogg_err2</th>\n      <th>koi_srad</th>\n      <th>koi_srad_err1</th>\n      <th>koi_srad_err2</th>\n      <th>ra</th>\n      <th>dec</th>\n      <th>koi_kepmag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3563</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.548413</td>\n      <td>5.470000e-05</td>\n      <td>-5.470000e-05</td>\n      <td>139.064020</td>\n      <td>0.004110</td>\n      <td>-0.004110</td>\n      <td>...</td>\n      <td>-133</td>\n      <td>4.387</td>\n      <td>0.066</td>\n      <td>-0.123</td>\n      <td>1.092</td>\n      <td>0.181</td>\n      <td>-0.097</td>\n      <td>298.09543</td>\n      <td>44.737061</td>\n      <td>13.204</td>\n    </tr>\n    <tr>\n      <th>4099</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24.754385</td>\n      <td>1.365000e-04</td>\n      <td>-1.365000e-04</td>\n      <td>140.207320</td>\n      <td>0.004460</td>\n      <td>-0.004460</td>\n      <td>...</td>\n      <td>-144</td>\n      <td>4.519</td>\n      <td>0.078</td>\n      <td>-0.052</td>\n      <td>0.804</td>\n      <td>0.056</td>\n      <td>-0.076</td>\n      <td>295.73535</td>\n      <td>42.576248</td>\n      <td>15.514</td>\n    </tr>\n    <tr>\n      <th>5460</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.057336</td>\n      <td>1.230000e-07</td>\n      <td>-1.230000e-07</td>\n      <td>131.792007</td>\n      <td>0.000096</td>\n      <td>-0.000096</td>\n      <td>...</td>\n      <td>-140</td>\n      <td>4.594</td>\n      <td>0.054</td>\n      <td>-0.027</td>\n      <td>0.683</td>\n      <td>0.054</td>\n      <td>-0.060</td>\n      <td>292.18417</td>\n      <td>49.310040</td>\n      <td>15.414</td>\n    </tr>\n    <tr>\n      <th>1091</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>201.118319</td>\n      <td>1.461000e-03</td>\n      <td>-1.461000e-03</td>\n      <td>187.569860</td>\n      <td>0.005290</td>\n      <td>-0.005290</td>\n      <td>...</td>\n      <td>-112</td>\n      <td>4.447</td>\n      <td>0.072</td>\n      <td>-0.108</td>\n      <td>0.954</td>\n      <td>0.135</td>\n      <td>-0.083</td>\n      <td>283.11377</td>\n      <td>48.131390</td>\n      <td>13.328</td>\n    </tr>\n    <tr>\n      <th>5999</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>91.649983</td>\n      <td>3.181000e-03</td>\n      <td>-3.181000e-03</td>\n      <td>175.715600</td>\n      <td>0.028600</td>\n      <td>-0.028600</td>\n      <td>...</td>\n      <td>-233</td>\n      <td>4.145</td>\n      <td>0.164</td>\n      <td>-0.164</td>\n      <td>1.608</td>\n      <td>0.905</td>\n      <td>-0.383</td>\n      <td>294.93198</td>\n      <td>39.812420</td>\n      <td>12.964</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 40 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Scale the data using the MinMaxScaler and perform some feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale your data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# One-hot encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "00\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2113 - accuracy: 0.9089\n",
      "Epoch 816/1000\n",
      "164/164 [==============================] - 0s 880us/step - loss: 0.2077 - accuracy: 0.9068\n",
      "Epoch 817/1000\n",
      "164/164 [==============================] - 0s 887us/step - loss: 0.2062 - accuracy: 0.9151\n",
      "Epoch 818/1000\n",
      "164/164 [==============================] - 0s 870us/step - loss: 0.2096 - accuracy: 0.9105\n",
      "Epoch 819/1000\n",
      "164/164 [==============================] - 0s 862us/step - loss: 0.2128 - accuracy: 0.9073\n",
      "Epoch 820/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2034 - accuracy: 0.9154\n",
      "Epoch 821/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2129 - accuracy: 0.9059\n",
      "Epoch 822/1000\n",
      "164/164 [==============================] - 0s 880us/step - loss: 0.2022 - accuracy: 0.9109\n",
      "Epoch 823/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2150 - accuracy: 0.9052\n",
      "Epoch 824/1000\n",
      "164/164 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9110\n",
      "Epoch 825/1000\n",
      "164/164 [==============================] - 0s 840us/step - loss: 0.2021 - accuracy: 0.9165\n",
      "Epoch 826/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2092 - accuracy: 0.9108\n",
      "Epoch 827/1000\n",
      "164/164 [==============================] - 0s 873us/step - loss: 0.2172 - accuracy: 0.9034\n",
      "Epoch 828/1000\n",
      "164/164 [==============================] - 0s 899us/step - loss: 0.2013 - accuracy: 0.9131\n",
      "Epoch 829/1000\n",
      "164/164 [==============================] - 0s 867us/step - loss: 0.2111 - accuracy: 0.9124\n",
      "Epoch 830/1000\n",
      "164/164 [==============================] - 0s 867us/step - loss: 0.2235 - accuracy: 0.9057\n",
      "Epoch 831/1000\n",
      "164/164 [==============================] - 0s 860us/step - loss: 0.2098 - accuracy: 0.9104\n",
      "Epoch 832/1000\n",
      "164/164 [==============================] - 0s 859us/step - loss: 0.2104 - accuracy: 0.9058\n",
      "Epoch 833/1000\n",
      "164/164 [==============================] - 0s 874us/step - loss: 0.2048 - accuracy: 0.9130\n",
      "Epoch 834/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2141 - accuracy: 0.9039\n",
      "Epoch 835/1000\n",
      "164/164 [==============================] - 0s 870us/step - loss: 0.2147 - accuracy: 0.9121\n",
      "Epoch 836/1000\n",
      "164/164 [==============================] - 0s 880us/step - loss: 0.2041 - accuracy: 0.9059\n",
      "Epoch 837/1000\n",
      "164/164 [==============================] - 0s 882us/step - loss: 0.2067 - accuracy: 0.9126\n",
      "Epoch 838/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2124 - accuracy: 0.9111\n",
      "Epoch 839/1000\n",
      "164/164 [==============================] - 0s 887us/step - loss: 0.2203 - accuracy: 0.9026\n",
      "Epoch 840/1000\n",
      "164/164 [==============================] - 0s 1ms/step - loss: 0.2143 - accuracy: 0.9134\n",
      "Epoch 841/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2033 - accuracy: 0.9112\n",
      "Epoch 842/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2093 - accuracy: 0.9048\n",
      "Epoch 843/1000\n",
      "164/164 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9083\n",
      "Epoch 844/1000\n",
      "164/164 [==============================] - 0s 875us/step - loss: 0.2169 - accuracy: 0.9026\n",
      "Epoch 845/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.1990 - accuracy: 0.9134\n",
      "Epoch 846/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2022 - accuracy: 0.9114\n",
      "Epoch 847/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2004 - accuracy: 0.9148\n",
      "Epoch 848/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2215 - accuracy: 0.9025\n",
      "Epoch 849/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2121 - accuracy: 0.9104\n",
      "Epoch 850/1000\n",
      "164/164 [==============================] - 0s 880us/step - loss: 0.2145 - accuracy: 0.9041\n",
      "Epoch 851/1000\n",
      "164/164 [==============================] - 0s 852us/step - loss: 0.2144 - accuracy: 0.9059\n",
      "Epoch 852/1000\n",
      "164/164 [==============================] - 0s 879us/step - loss: 0.2094 - accuracy: 0.9069\n",
      "Epoch 853/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2092 - accuracy: 0.9110\n",
      "Epoch 854/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2110 - accuracy: 0.9060\n",
      "Epoch 855/1000\n",
      "164/164 [==============================] - 0s 874us/step - loss: 0.2116 - accuracy: 0.9089\n",
      "Epoch 856/1000\n",
      "164/164 [==============================] - 0s 916us/step - loss: 0.1999 - accuracy: 0.9122\n",
      "Epoch 857/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2133 - accuracy: 0.9058\n",
      "Epoch 858/1000\n",
      "164/164 [==============================] - 0s 866us/step - loss: 0.2097 - accuracy: 0.9043\n",
      "Epoch 859/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2152 - accuracy: 0.9053\n",
      "Epoch 860/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2091 - accuracy: 0.9113\n",
      "Epoch 861/1000\n",
      "164/164 [==============================] - 0s 882us/step - loss: 0.2065 - accuracy: 0.9091\n",
      "Epoch 862/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2106 - accuracy: 0.9116\n",
      "Epoch 863/1000\n",
      "164/164 [==============================] - 0s 963us/step - loss: 0.2024 - accuracy: 0.9107\n",
      "Epoch 864/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2094 - accuracy: 0.9059\n",
      "Epoch 865/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.1996 - accuracy: 0.9135\n",
      "Epoch 866/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2046 - accuracy: 0.9136\n",
      "Epoch 867/1000\n",
      "164/164 [==============================] - 0s 865us/step - loss: 0.2020 - accuracy: 0.9125\n",
      "Epoch 868/1000\n",
      "164/164 [==============================] - 0s 875us/step - loss: 0.2018 - accuracy: 0.9170\n",
      "Epoch 869/1000\n",
      "164/164 [==============================] - 0s 879us/step - loss: 0.2044 - accuracy: 0.9089\n",
      "Epoch 870/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2074 - accuracy: 0.9070\n",
      "Epoch 871/1000\n",
      "164/164 [==============================] - 0s 894us/step - loss: 0.2197 - accuracy: 0.9050\n",
      "Epoch 872/1000\n",
      "164/164 [==============================] - 0s 875us/step - loss: 0.2170 - accuracy: 0.9088\n",
      "Epoch 873/1000\n",
      "164/164 [==============================] - 0s 880us/step - loss: 0.2020 - accuracy: 0.9095\n",
      "Epoch 874/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2113 - accuracy: 0.9055\n",
      "Epoch 875/1000\n",
      "164/164 [==============================] - 0s 888us/step - loss: 0.2202 - accuracy: 0.9016\n",
      "Epoch 876/1000\n",
      "164/164 [==============================] - 0s 890us/step - loss: 0.2136 - accuracy: 0.9077\n",
      "Epoch 877/1000\n",
      "164/164 [==============================] - 0s 866us/step - loss: 0.2023 - accuracy: 0.9127\n",
      "Epoch 878/1000\n",
      "164/164 [==============================] - 0s 854us/step - loss: 0.2032 - accuracy: 0.9115\n",
      "Epoch 879/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2063 - accuracy: 0.9108\n",
      "Epoch 880/1000\n",
      "164/164 [==============================] - 0s 874us/step - loss: 0.2038 - accuracy: 0.9149\n",
      "Epoch 881/1000\n",
      "164/164 [==============================] - 0s 868us/step - loss: 0.2082 - accuracy: 0.9086\n",
      "Epoch 882/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2012 - accuracy: 0.9156\n",
      "Epoch 883/1000\n",
      "164/164 [==============================] - 0s 987us/step - loss: 0.2002 - accuracy: 0.9117\n",
      "Epoch 884/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2081 - accuracy: 0.9102\n",
      "Epoch 885/1000\n",
      "164/164 [==============================] - 0s 885us/step - loss: 0.2062 - accuracy: 0.9145\n",
      "Epoch 886/1000\n",
      "164/164 [==============================] - 0s 881us/step - loss: 0.2037 - accuracy: 0.9131\n",
      "Epoch 887/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2131 - accuracy: 0.9082\n",
      "Epoch 888/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2055 - accuracy: 0.9171\n",
      "Epoch 889/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2070 - accuracy: 0.9050\n",
      "Epoch 890/1000\n",
      "164/164 [==============================] - 0s 914us/step - loss: 0.1956 - accuracy: 0.9164\n",
      "Epoch 891/1000\n",
      "164/164 [==============================] - 0s 860us/step - loss: 0.2047 - accuracy: 0.9115\n",
      "Epoch 892/1000\n",
      "164/164 [==============================] - 0s 874us/step - loss: 0.2089 - accuracy: 0.9063\n",
      "Epoch 893/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2211 - accuracy: 0.9037\n",
      "Epoch 894/1000\n",
      "164/164 [==============================] - 0s 859us/step - loss: 0.2012 - accuracy: 0.9129\n",
      "Epoch 895/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2180 - accuracy: 0.9034\n",
      "Epoch 896/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2072 - accuracy: 0.9121\n",
      "Epoch 897/1000\n",
      "164/164 [==============================] - 0s 914us/step - loss: 0.2065 - accuracy: 0.9110\n",
      "Epoch 898/1000\n",
      "164/164 [==============================] - 0s 865us/step - loss: 0.2070 - accuracy: 0.9093\n",
      "Epoch 899/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2064 - accuracy: 0.9115\n",
      "Epoch 900/1000\n",
      "164/164 [==============================] - 0s 861us/step - loss: 0.2156 - accuracy: 0.9016\n",
      "Epoch 901/1000\n",
      "164/164 [==============================] - 0s 867us/step - loss: 0.2102 - accuracy: 0.9074\n",
      "Epoch 902/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2121 - accuracy: 0.9015\n",
      "Epoch 903/1000\n",
      "164/164 [==============================] - 0s 882us/step - loss: 0.1991 - accuracy: 0.9180\n",
      "Epoch 904/1000\n",
      "164/164 [==============================] - 0s 969us/step - loss: 0.2166 - accuracy: 0.9044\n",
      "Epoch 905/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.1958 - accuracy: 0.9132\n",
      "Epoch 906/1000\n",
      "164/164 [==============================] - 0s 888us/step - loss: 0.2121 - accuracy: 0.9059\n",
      "Epoch 907/1000\n",
      "164/164 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9119\n",
      "Epoch 908/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2072 - accuracy: 0.9090\n",
      "Epoch 909/1000\n",
      "164/164 [==============================] - 0s 896us/step - loss: 0.2061 - accuracy: 0.9092\n",
      "Epoch 910/1000\n",
      "164/164 [==============================] - 0s 869us/step - loss: 0.2068 - accuracy: 0.9103\n",
      "Epoch 911/1000\n",
      "164/164 [==============================] - 0s 872us/step - loss: 0.2087 - accuracy: 0.9123\n",
      "Epoch 912/1000\n",
      "164/164 [==============================] - 0s 862us/step - loss: 0.2143 - accuracy: 0.9061\n",
      "Epoch 913/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2030 - accuracy: 0.9137\n",
      "Epoch 914/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2042 - accuracy: 0.9093\n",
      "Epoch 915/1000\n",
      "164/164 [==============================] - 0s 908us/step - loss: 0.2174 - accuracy: 0.9065\n",
      "Epoch 916/1000\n",
      "164/164 [==============================] - 0s 887us/step - loss: 0.2119 - accuracy: 0.9076\n",
      "Epoch 917/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2056 - accuracy: 0.9137\n",
      "Epoch 918/1000\n",
      "164/164 [==============================] - 0s 880us/step - loss: 0.2035 - accuracy: 0.9118\n",
      "Epoch 919/1000\n",
      "164/164 [==============================] - 0s 858us/step - loss: 0.2054 - accuracy: 0.9085\n",
      "Epoch 920/1000\n",
      "164/164 [==============================] - 0s 876us/step - loss: 0.2053 - accuracy: 0.9088\n",
      "Epoch 921/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2063 - accuracy: 0.9159\n",
      "Epoch 922/1000\n",
      "164/164 [==============================] - 0s 844us/step - loss: 0.2244 - accuracy: 0.9050\n",
      "Epoch 923/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2009 - accuracy: 0.9137\n",
      "Epoch 924/1000\n",
      "164/164 [==============================] - 0s 957us/step - loss: 0.2108 - accuracy: 0.9030\n",
      "Epoch 925/1000\n",
      "164/164 [==============================] - 0s 858us/step - loss: 0.2011 - accuracy: 0.9149\n",
      "Epoch 926/1000\n",
      "164/164 [==============================] - 0s 865us/step - loss: 0.2110 - accuracy: 0.8997\n",
      "Epoch 927/1000\n",
      "164/164 [==============================] - 0s 957us/step - loss: 0.2135 - accuracy: 0.9075\n",
      "Epoch 928/1000\n",
      "164/164 [==============================] - 0s 847us/step - loss: 0.2125 - accuracy: 0.9041\n",
      "Epoch 929/1000\n",
      "164/164 [==============================] - 0s 892us/step - loss: 0.2050 - accuracy: 0.9104\n",
      "Epoch 930/1000\n",
      "164/164 [==============================] - 0s 878us/step - loss: 0.1960 - accuracy: 0.9150\n",
      "Epoch 931/1000\n",
      "164/164 [==============================] - 0s 881us/step - loss: 0.2105 - accuracy: 0.9146\n",
      "Epoch 932/1000\n",
      "164/164 [==============================] - 0s 885us/step - loss: 0.2147 - accuracy: 0.9050\n",
      "Epoch 933/1000\n",
      "164/164 [==============================] - 0s 891us/step - loss: 0.2131 - accuracy: 0.9082\n",
      "Epoch 934/1000\n",
      "164/164 [==============================] - 0s 879us/step - loss: 0.2164 - accuracy: 0.9035\n",
      "Epoch 935/1000\n",
      "164/164 [==============================] - 0s 865us/step - loss: 0.2026 - accuracy: 0.9115\n",
      "Epoch 936/1000\n",
      "164/164 [==============================] - 0s 896us/step - loss: 0.2082 - accuracy: 0.9092\n",
      "Epoch 937/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2161 - accuracy: 0.9048\n",
      "Epoch 938/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2097 - accuracy: 0.9073\n",
      "Epoch 939/1000\n",
      "164/164 [==============================] - 0s 870us/step - loss: 0.1995 - accuracy: 0.9156\n",
      "Epoch 940/1000\n",
      "164/164 [==============================] - 0s 880us/step - loss: 0.2157 - accuracy: 0.9056\n",
      "Epoch 941/1000\n",
      "164/164 [==============================] - 0s 891us/step - loss: 0.2135 - accuracy: 0.9063\n",
      "Epoch 942/1000\n",
      "164/164 [==============================] - 0s 876us/step - loss: 0.2030 - accuracy: 0.9055\n",
      "Epoch 943/1000\n",
      "164/164 [==============================] - 0s 862us/step - loss: 0.2082 - accuracy: 0.9093\n",
      "Epoch 944/1000\n",
      "164/164 [==============================] - 0s 938us/step - loss: 0.2163 - accuracy: 0.9030\n",
      "Epoch 945/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2120 - accuracy: 0.9049\n",
      "Epoch 946/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2094 - accuracy: 0.9088\n",
      "Epoch 947/1000\n",
      "164/164 [==============================] - 0s 869us/step - loss: 0.2085 - accuracy: 0.9139\n",
      "Epoch 948/1000\n",
      "164/164 [==============================] - 0s 875us/step - loss: 0.2074 - accuracy: 0.9057\n",
      "Epoch 949/1000\n",
      "164/164 [==============================] - 0s 895us/step - loss: 0.2063 - accuracy: 0.9104\n",
      "Epoch 950/1000\n",
      "164/164 [==============================] - 0s 865us/step - loss: 0.2024 - accuracy: 0.9122\n",
      "Epoch 951/1000\n",
      "164/164 [==============================] - 0s 861us/step - loss: 0.2111 - accuracy: 0.9124\n",
      "Epoch 952/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2167 - accuracy: 0.9080\n",
      "Epoch 953/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.1973 - accuracy: 0.9142\n",
      "Epoch 954/1000\n",
      "164/164 [==============================] - 0s 864us/step - loss: 0.2052 - accuracy: 0.9123\n",
      "Epoch 955/1000\n",
      "164/164 [==============================] - 0s 898us/step - loss: 0.2011 - accuracy: 0.9120\n",
      "Epoch 956/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2157 - accuracy: 0.9066\n",
      "Epoch 957/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2076 - accuracy: 0.9104\n",
      "Epoch 958/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2067 - accuracy: 0.9086\n",
      "Epoch 959/1000\n",
      "164/164 [==============================] - 0s 881us/step - loss: 0.2098 - accuracy: 0.9098\n",
      "Epoch 960/1000\n",
      "164/164 [==============================] - 0s 882us/step - loss: 0.2089 - accuracy: 0.9136\n",
      "Epoch 961/1000\n",
      "164/164 [==============================] - 0s 861us/step - loss: 0.2137 - accuracy: 0.9063\n",
      "Epoch 962/1000\n",
      "164/164 [==============================] - 0s 880us/step - loss: 0.2017 - accuracy: 0.9136\n",
      "Epoch 963/1000\n",
      "164/164 [==============================] - 0s 905us/step - loss: 0.2038 - accuracy: 0.9104\n",
      "Epoch 964/1000\n",
      "164/164 [==============================] - 0s 979us/step - loss: 0.1996 - accuracy: 0.9156\n",
      "Epoch 965/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2045 - accuracy: 0.9090\n",
      "Epoch 966/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2086 - accuracy: 0.9066\n",
      "Epoch 967/1000\n",
      "164/164 [==============================] - 0s 881us/step - loss: 0.2068 - accuracy: 0.9092\n",
      "Epoch 968/1000\n",
      "164/164 [==============================] - 0s 881us/step - loss: 0.2036 - accuracy: 0.9091\n",
      "Epoch 969/1000\n",
      "164/164 [==============================] - 0s 901us/step - loss: 0.2043 - accuracy: 0.9125\n",
      "Epoch 970/1000\n",
      "164/164 [==============================] - 0s 875us/step - loss: 0.2044 - accuracy: 0.9125\n",
      "Epoch 971/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2047 - accuracy: 0.9127\n",
      "Epoch 972/1000\n",
      "164/164 [==============================] - 0s 872us/step - loss: 0.2081 - accuracy: 0.9091\n",
      "Epoch 973/1000\n",
      "164/164 [==============================] - 0s 858us/step - loss: 0.2219 - accuracy: 0.8941\n",
      "Epoch 974/1000\n",
      "164/164 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9145\n",
      "Epoch 975/1000\n",
      "164/164 [==============================] - 0s 875us/step - loss: 0.2020 - accuracy: 0.9092\n",
      "Epoch 976/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.1993 - accuracy: 0.9135\n",
      "Epoch 977/1000\n",
      "164/164 [==============================] - 0s 888us/step - loss: 0.2061 - accuracy: 0.9117\n",
      "Epoch 978/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.2148 - accuracy: 0.9068\n",
      "Epoch 979/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2066 - accuracy: 0.9113\n",
      "Epoch 980/1000\n",
      "164/164 [==============================] - 0s 901us/step - loss: 0.2150 - accuracy: 0.9070\n",
      "Epoch 981/1000\n",
      "164/164 [==============================] - 0s 865us/step - loss: 0.2212 - accuracy: 0.9039\n",
      "Epoch 982/1000\n",
      "164/164 [==============================] - 0s 878us/step - loss: 0.1976 - accuracy: 0.9088\n",
      "Epoch 983/1000\n",
      "164/164 [==============================] - 0s 876us/step - loss: 0.1968 - accuracy: 0.9120\n",
      "Epoch 984/1000\n",
      "164/164 [==============================] - 0s 969us/step - loss: 0.2135 - accuracy: 0.9055\n",
      "Epoch 985/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2060 - accuracy: 0.9155\n",
      "Epoch 986/1000\n",
      "164/164 [==============================] - 0s 877us/step - loss: 0.2008 - accuracy: 0.9146\n",
      "Epoch 987/1000\n",
      "164/164 [==============================] - 0s 865us/step - loss: 0.1954 - accuracy: 0.9135\n",
      "Epoch 988/1000\n",
      "164/164 [==============================] - 0s 884us/step - loss: 0.2191 - accuracy: 0.8990\n",
      "Epoch 989/1000\n",
      "164/164 [==============================] - 0s 879us/step - loss: 0.1992 - accuracy: 0.9120\n",
      "Epoch 990/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2190 - accuracy: 0.9040\n",
      "Epoch 991/1000\n",
      "164/164 [==============================] - 0s 876us/step - loss: 0.2133 - accuracy: 0.9061\n",
      "Epoch 992/1000\n",
      "164/164 [==============================] - 0s 889us/step - loss: 0.1973 - accuracy: 0.9144\n",
      "Epoch 993/1000\n",
      "164/164 [==============================] - 0s 883us/step - loss: 0.2099 - accuracy: 0.9065\n",
      "Epoch 994/1000\n",
      "164/164 [==============================] - 0s 882us/step - loss: 0.2118 - accuracy: 0.9081\n",
      "Epoch 995/1000\n",
      "164/164 [==============================] - 0s 863us/step - loss: 0.2080 - accuracy: 0.9099\n",
      "Epoch 996/1000\n",
      "164/164 [==============================] - 0s 867us/step - loss: 0.1974 - accuracy: 0.9118\n",
      "Epoch 997/1000\n",
      "164/164 [==============================] - 0s 892us/step - loss: 0.2138 - accuracy: 0.9102\n",
      "Epoch 998/1000\n",
      "164/164 [==============================] - 0s 871us/step - loss: 0.2048 - accuracy: 0.9152\n",
      "Epoch 999/1000\n",
      "164/164 [==============================] - 0s 869us/step - loss: 0.2087 - accuracy: 0.9097\n",
      "Epoch 1000/1000\n",
      "164/164 [==============================] - 0s 876us/step - loss: 0.2186 - accuracy: 0.9048\n",
      "164/164 [==============================] - 1s 803us/step - loss: 0.2023 - accuracy: 0.9134\n",
      "Train - Loss: 0.2023075520992279, Accuracy: 0.9134083390235901\n",
      "55/55 [==============================] - 0s 759us/step - loss: 0.3038 - accuracy: 0.9045\n",
      "Test - Loss: 0.3037976920604706, Accuracy: 0.9044622182846069\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "number_inputs = len(X.columns)\n",
    "number_hidden_nodes = number_inputs + 1\n",
    "number_outputs = len(y_train_categorical[0])\n",
    "print(number_inputs, number_hidden_nodes, number_outputs)\n",
    "\n",
    "model.add(Dense(units=number_hidden_nodes, activation='relu', input_dim=number_inputs))\n",
    "model.add(Dense(units=number_outputs, activation='softmax'))\n",
    "\n",
    "#GridSearch (Gradient Descent) is being done in .compile or .fit)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_scaled, y_train_categorical, epochs=1000,shuffle=True)\n",
    "\n",
    "model_loss, model_accuracy = model.evaluate(X_train_scaled, y_train_categorical)\n",
    "print(f\"Train - Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical)\n",
    "print(f\"Test - Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "#print(f\"Training Data Score: {model.score(X_train_scaled, y_train)}\")\n",
    "#print(f\"Testing Data Score: {model.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "model.save('neural.h5')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "dev"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a7672e96ae4814148c9df53f13e13cee1f7b81f4499d63bd6bb607e17576b5f7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}